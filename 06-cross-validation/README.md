# Cross Validation

Cross-validation calculates errors across multiple train test splits, which is better than calculating errors on just one single fest split.

This covers you so that you don't have to worry about just getting a model correct by luck, but that model is able to perform well for any given holdout set.

Error matrix related to the complexity when determining the model:

![error.png](images/error.png)

Now, when you do cross-validationÂ and you have each one of your holdout or validation sets,Â the idea will be that you will be able to decreaseÂ error up to a certain point by making it more complex,Â but as you get more and more complex,Â there will be an inflection pointÂ where you'll start to be over-fitting toÂ your data and it won't generalize well to new data.Â So models associated withÂ the left side of this curve beforeÂ we hit the plateau are considered to be under-fitting,Â so they are not complex enough. Models associated with the right side ofÂ this curve are going to beÂ considered over-fitting the data.

> ðŸ’¡ Our goal is to get this just the right point where both the cross-validation error and training error is relatively low.

## Cross-Validation Approaches

### K-fold Cross-validation:

In this method, we split the data-set into k number of subsets(known as folds) then we perform training on the all the subsets but leave one(k-1) subset for the evaluation of the trained model.

### Leave One Out Cross Validation:

That's going to be similar to k-fold,Â just this time k is going to beÂ equal to the number of rows minus 1,Â where we're going toÂ leave one out for every single train test,Â split just a single row, and train on the remaining rows.Â So the plus of theseÂ is that we'll get many more test sets,Â many more evaluations of the test setÂ so that we can be more certain about how wellÂ we're able to actually predict new incoming data.Â But it'll take a lot longerÂ to actually train our data as we haveÂ to train across the number of rows that there are.

### Stratified Cross-Validation:

K-fold Cross-validation with representative samples

- What do you mean by representative sample?
    
    If we are working withÂ a categorical outcome variable such as true orÂ false and there was 80 percentÂ true and 20 percent false on our original dataset,Â we want to ensure that when we doÂ our train and test splits that our train andÂ test splits also have a breakdown ofÂ 80 percent true, 20 percent false.

## Cross-validation: The Syntax

- Import the cross-validation function

```
from sklearn.model_selection import cross_val_score
```

- Other methods for cross-validation

```
from sklearn.model_selection import KFold, StratifiedKFold
```

- Perform cross-validation with a given model

```
cross_val = cross_val_score(model, X_data, y_data, cv=4, scoring='neg_mean_squared_error')
```

Here, cv is the number of splits

We're going to want to maximizeÂ whatever that scoring value is.Â So obviously, we want to minimize mean squared error.Â So we have to use negative mean squared error andÂ it'll maximize the negative mean squared error,Â which means we are minimizing mean squared error.